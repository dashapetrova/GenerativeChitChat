{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wZup1NkebabS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da34eec5-51ce-4076-b166-14aa529b63c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') #, force_remount=True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('gdrive/My Drive/Colab Notebooks/ml_2')"
      ],
      "metadata": {
        "id": "4lyUkPq8aNH3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip train.txt.zip"
      ],
      "metadata": {
        "id": "tUx6BVcJbQqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "id": "bv2_wQe6bwYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ff2ced-08d4-422e-8342-abaf26b7eac5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext import datasets\n",
        "from torchtext.legacy import data\n",
        "from tqdm import tqdm\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "import math, copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "X1Cn2P5GbgWf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_nLUutLboPn",
        "outputId": "e8113b5c-02d1-44c5-a51f-818614eba02c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка данных"
      ],
      "metadata": {
        "id": "j4ecOBNQOi7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data = open('train.txt').readlines()"
      ],
      "metadata": {
        "id": "vMtJsyiEb-dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"f1 = open('questions.txt', 'w', encoding='utf-8')\n",
        "f2 = open('answers.txt', 'w', encoding='utf-8')\n",
        "for line in data:\n",
        "  line = line.strip('\\n')\n",
        "  question, answer = line.split('\\t')\n",
        "  f1.write(question + '\\n')\n",
        "  f2.write(answer + '\\n')\n",
        "f1.close()\n",
        "f2.close()\"\"\""
      ],
      "metadata": {
        "id": "CYJpshSTANFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"tokenizer = Tokenizer(BPE())\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[BOS]\", \"[EOS]\", \"[PAD]\"])\n",
        "tokenizer.train(files=['questions.txt', 'answers.txt'], trainer=trainer)\"\"\""
      ],
      "metadata": {
        "id": "-YoNe9YjAPjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer.save('tokenizer')"
      ],
      "metadata": {
        "id": "fdJNAd-sr2w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file(\"tokenizer\")"
      ],
      "metadata": {
        "id": "hj8vKrrDy-XS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA = data.Field(\n",
        "    fix_length=50,\n",
        "    unk_token='[UNK]',\n",
        "    init_token='[BOS]',\n",
        "    eos_token='[EOS]',\n",
        "    pad_token='[PAD]',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tokenizer.encode(x).tokens,\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "fields = (('src', DATA), ('tgt', DATA))"
      ],
      "metadata": {
        "id": "AM1BiNQ_1mz9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('questions.txt') as f:\n",
        "    src_snt = list(map(str.strip, f.readlines()))\n",
        "    \n",
        "with open('answers.txt') as f:\n",
        "    tgt_snt = list(map(str.strip, f.readlines()))"
      ],
      "metadata": {
        "id": "NuVjHwGg2Q0R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(src_snt), len(tgt_snt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQU5aqz2ENh6",
        "outputId": "979be274-1372-4294-ff16-5972e59d9c4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5880497, 5880497)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В качестве фильтрации и способа уменьшить число данных возьмем только четные элементы и ограничимся 20000 парами вопросов и ответов"
      ],
      "metadata": {
        "id": "GY5OygKxFDoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_snt_new = src_snt[::2][:20000]\n",
        "tgt_snt_new = tgt_snt[::2][:20000]"
      ],
      "metadata": {
        "id": "h_NSFLWvEaVa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(src_snt_new), len(tgt_snt_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6OE_GCGFQUp",
        "outputId": "42b9d925-a597-4cb6-c95f-dd39f49e5648"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = [data.Example.fromlist(x, fields) for x in tqdm(zip(src_snt_new, tgt_snt_new), total=len(tgt_snt_new))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0gHTHbn4-eg",
        "outputId": "596657c5-d918-4a4e-f463-0b71b01f13b8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20000/20000 [00:02<00:00, 6900.86it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = train_test_split(all_data, test_size=0.05)"
      ],
      "metadata": {
        "id": "2AZLe_ALHGiP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, valid_data = train_test_split(train_data, test_size=0.1)"
      ],
      "metadata": {
        "id": "gsQor8AoKUGH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = data.Dataset(train_data, fields)\n",
        "valid_set = data.Dataset(valid_data, fields)\n",
        "test_set = data.Dataset(test_data, fields)"
      ],
      "metadata": {
        "id": "AeOkAsuR68Vi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA.build_vocab(train_set)\n",
        "DATA.vocab.stoi=tokenizer.get_vocab()\n",
        "DATA.vocab.itos = {value: key for key, value in tokenizer.get_vocab().items()}"
      ],
      "metadata": {
        "id": "8aKjWJ6T1xsH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = tokenizer.token_to_id('[PAD]')\n",
        "PAD_IDX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7j5PQT-Fz-A",
        "outputId": "3a4484ca-10a5-4844-c57e-98dd25a342ae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(DATA.vocab)"
      ],
      "metadata": {
        "id": "AqIBgVLZkM54"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Реализация модели"
      ],
      "metadata": {
        "id": "ON7dNN77Odx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 200):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
      ],
      "metadata": {
        "id": "wZUNlbjwc9sh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ],
      "metadata": {
        "id": "8N-2_ugjdyHX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2seqTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(Seq2seqTransformer, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ],
      "metadata": {
        "id": "z_t82r69Nc3d"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  \n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "metadata": {
        "id": "rTdH5WBrRawM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        \n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "metadata": {
        "id": "9J8lWS5zSGig"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "            \n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "lqzqq7gbRyq3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "6vYrzcRkSAq4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, size, self_attention, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attention = self_attention\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = nn.ModuleList([copy.deepcopy(SublayerConnection(size, dropout)) for _ in range(2)])\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attention(x, x, x, mask))\n",
        "        \n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "mkGsGSxcSYP3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "            \n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "QHnf9YjnSrSE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = nn.ModuleList([copy.deepcopy(SublayerConnection(size, dropout)) for _ in range(3)])\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\n",
        "        \n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "ywKjwBjES0p2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "        \n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "metadata": {
        "id": "AONdEyw1Ts5Y"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, emb_size, dropout=0.1):\n",
        "\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        self.d_k = emb_size // h\n",
        "        self.h = h\n",
        "        self.linears = nn.ModuleList([copy.deepcopy(nn.Linear(emb_size, emb_size)) for _ in range(4)])\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "        x = self.linears[-1](x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "SyP9oxXVUA9c"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \n",
        "    def __init__(self, emb_size, hidden_size, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.lr_1 = nn.Linear(emb_size, hidden_size)\n",
        "        self.lr_2 = nn.Linear(hidden_size, emb_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lr_2(self.dropout(F.relu(self.lr_1(x))))"
      ],
      "metadata": {
        "id": "udLNTfGYUVwq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Батчи и маскирование"
      ],
      "metadata": {
        "id": "cSjg7FTpYEkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subsequent_mask(size):    \n",
        "  \n",
        "    subsequent_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
        "\n",
        "    return torch.from_numpy(subsequent_mask) == 0"
      ],
      "metadata": {
        "id": "XdKnpfxfbDeN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Batch:\n",
        "    \n",
        "    def __init__(self, src, trg=None, pad=PAD_IDX):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = self.make_std_mask(self.trg, pad)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        \n",
        "        return tgt_mask"
      ],
      "metadata": {
        "id": "IKOJ6rByYAnQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interator и criterion"
      ],
      "metadata": {
        "id": "jd8qHB-2YdOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BucketIteratorWrapper(DataLoader):\n",
        "    __initialized = False\n",
        "\n",
        "    def __init__(self, iterator: data.Iterator):\n",
        "        #super(BucketIteratorWrapper,self).__init__()\n",
        "        self.batch_size = iterator.batch_size\n",
        "        self.num_workers = 8\n",
        "        self.collate_fn = None\n",
        "        self.pin_memory = False\n",
        "        self.drop_last = False\n",
        "        self.timeout = 0\n",
        "        self.worker_init_fn = None\n",
        "        self.sampler = iterator\n",
        "        self.batch_sampler = iterator\n",
        "        self.__initialized = True\n",
        "\n",
        "    def __iter__(self):\n",
        "        return map(lambda batch: Batch(batch.src, batch.tgt, pad=PAD_IDX),\n",
        "            self.batch_sampler.__iter__())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batch_sampler)\n",
        "    \n",
        "class MyCriterion(nn.Module):\n",
        "    def __init__(self, generator, pad_idx):\n",
        "        super(MyCriterion, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.pad_idx = pad_idx\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_idx)\n",
        "        self.criterion.cuda()\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        ntokens = (target != self.pad_idx).data.sum()\n",
        "        x = self.generator(x)\n",
        "\n",
        "        return self.criterion(x.reshape(-1, x.size(-1)), target.reshape(-1))  / ntokens"
      ],
      "metadata": {
        "id": "-xO37pl6YPum"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head_num=8\n",
        "emb_size=512\n",
        "hidden_size=512\n",
        "dropout=0.1\n",
        "N=4\n",
        "\n",
        "c = copy.deepcopy\n",
        "attn = MultiHeadedAttention(head_num, emb_size)\n",
        "ff = PositionwiseFeedForward(emb_size, hidden_size, dropout)\n",
        "position = PositionalEncoding(emb_size, dropout)\n",
        "model = Seq2seqTransformer(\n",
        "    Encoder(EncoderLayer(emb_size, c(attn), c(ff), dropout), N),\n",
        "    Decoder(DecoderLayer(emb_size, c(attn), c(attn), c(ff), dropout), N),\n",
        "    nn.Sequential(TokenEmbedding(vocab_size, emb_size), c(position)),\n",
        "    nn.Sequential(TokenEmbedding(vocab_size, emb_size), c(position)),\n",
        "    Generator(emb_size, vocab_size))\n",
        "    \n",
        "for p in model.parameters():\n",
        "  if p.dim() > 1:\n",
        "    nn.init.xavier_uniform(p)\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = MyCriterion(generator=model.generator, pad_idx=PAD_IDX)\n",
        "criterion = criterion.to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXAASGyGnpwG",
        "outputId": "a7dc2aa9-6e13-407c-e715-d01c48f8764e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение модели"
      ],
      "metadata": {
        "id": "hCUfa5IIY7m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=128\n",
        "\n",
        "train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_set, valid_set, test_set), \n",
        "                                              batch_sizes=(batch_size, batch_size, batch_size), \n",
        "                                  sort_key=lambda x: len(x.src), shuffle=True,\n",
        "                                  device=DEVICE, sort_within_batch=False)\n",
        "\n",
        "train_iter = BucketIteratorWrapper(train_iter)\n",
        "valid_iter = BucketIteratorWrapper(valid_iter)\n",
        "test_iter = BucketIteratorWrapper(test_iter)"
      ],
      "metadata": {
        "id": "Kdt2m453tghR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(iterator, model, criterion, optimizer):\n",
        "    total_loss = 0\n",
        "    counter = 0\n",
        "\n",
        "    iterator = tqdm(enumerate(iterator), total=len(iterator), desc=\"Training\")\n",
        "    for i, batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
        "        loss = criterion(pred, batch.trg_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        curr_loss = loss.detach().item()\n",
        "\n",
        "        total_loss += curr_loss\n",
        "        iterator.set_postfix(loss = curr_loss)\n",
        "        counter +=1\n",
        "        \n",
        "    total_loss /= counter\n",
        "\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "ZHxG0-iLY2f6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(iterator, model, criterion):\n",
        "    total_loss = 0\n",
        "    counter = 0\n",
        "\n",
        "    iterator = tqdm(enumerate(iterator), total=len(iterator))\n",
        "    for i, batch in iterator:\n",
        "        pred = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
        "        loss = criterion(pred, batch.trg_y)\n",
        "        \n",
        "        curr_loss = loss.detach().item()\n",
        "        \n",
        "        total_loss += curr_loss\n",
        "        iterator.set_postfix(loss = curr_loss)\n",
        "        counter +=1\n",
        "        \n",
        "    total_loss /= counter\n",
        "\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "f_pvt4--p_nD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    loss = train(train_iter, model, criterion, optimizer)\n",
        "    print('train', loss)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss = evaluate(valid_iter, model, criterion)\n",
        "        scheduler.step(loss)\n",
        "        print('valid', loss)\n",
        "\n",
        "    torch.save(model, 'model_1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaqKKlTid5pq",
        "outputId": "aaad8dee-0ca7-45b6-8ef3-eb3140c8c24f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 134/134 [1:02:20<00:00, 27.91s/it, loss=6.95]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 7.1584875761573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [02:18<00:00,  9.21s/it, loss=6.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid 6.803585243225098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 134/134 [1:07:03<00:00, 30.02s/it, loss=6.47]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 6.547142014574649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [02:19<00:00,  9.30s/it, loss=6.82]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid 6.698142337799072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 134/134 [1:13:08<00:00, 32.75s/it, loss=6.41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 6.320283750989544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [02:19<00:00,  9.30s/it, loss=6.82]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid 6.66749890645345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 134/134 [1:12:46<00:00, 32.58s/it, loss=6.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 6.125271106833842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [02:23<00:00,  9.55s/it, loss=6.82]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid 6.67335786819458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 134/134 [1:22:52<00:00, 37.11s/it, loss=5.91]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 5.9362247915410284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [02:20<00:00,  9.39s/it, loss=6.86]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid 6.7193100611368815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  32%|███▏      | 43/134 [25:42<56:11, 37.05s/it, loss=5.84]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss на валидации начал возрастать, поэтому чтобы модель не переобучилась, обучение было остановлено"
      ],
      "metadata": {
        "id": "E4-FrBR7fmY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('model_1')"
      ],
      "metadata": {
        "id": "r_QA7CfFVwO2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Генерация ответа"
      ],
      "metadata": {
        "id": "B_CVE0lQgB_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для генерации ответа я написала две функции get_answer и get_answer_2: первая использует greedy search, top-k и top-p sampling, вторая - beam_search"
      ],
      "metadata": {
        "id": "ft7RGke56ERb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(question, method, max_len = 50):\n",
        "  input_ids = [tokenizer.token_to_id('[BOS]')] + tokenizer.encode(question).ids\n",
        "\n",
        "  if len(input_ids) > max_len:\n",
        "    input_ids[:max_len]\n",
        "\n",
        "  if len(input_ids) < max_len:      \n",
        "    diff = max_len - len(input_ids)\n",
        "    padding_idx = [PAD_IDX] * diff\n",
        "    input_ids.extend(padding_idx)\n",
        "  \n",
        "  input = torch.tensor(input_ids).unsqueeze(0)\n",
        "  input_mask = (input != PAD_IDX)\n",
        "\n",
        "  output_ids = []\n",
        "  output = torch.tensor([tokenizer.token_to_id('[BOS]')]).unsqueeze(0)\n",
        "  output_mask = torch.tensor([True])\n",
        "\n",
        "  with torch.no_grad():\n",
        "    input_encoded = model.encode(input, src_mask=input_mask)\n",
        "\n",
        "    while len(output_ids) < max_len:\n",
        "      pred = model.decode(input_encoded, input_mask, output, output_mask)\n",
        "      pred = model.generator(pred)\n",
        "      pred_id = decoding(pred, method=method)\n",
        "\n",
        "      output_ids.append(pred_id)\n",
        "      output = torch.cat((output, torch.tensor([pred_id]).unsqueeze(0).T), dim=1)\n",
        "  \n",
        "  return tokenizer.decode(output_ids)"
      ],
      "metadata": {
        "id": "F_FIcWIqWiIX"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoding(pred, method):\n",
        "\n",
        "  #greedy search\n",
        "  if method == 'argmax':\n",
        "    return argmax(pred)\n",
        "  \n",
        "  #top-k sampling\n",
        "  if method == 'top-k sampling':\n",
        "    return top_k_sampling(pred)\n",
        "\n",
        "  #top-p sampling\n",
        "  if method == 'top-p sampling':\n",
        "    return top_p_sampling(pred)"
      ],
      "metadata": {
        "id": "aGCP39d8rmyN"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def argmax(pred):\n",
        "  probs = torch.softmax(pred, dim=2)\n",
        "  pred_id = torch.argmax(probs[0, -1, :]).item()\n",
        "  return pred_id"
      ],
      "metadata": {
        "id": "vOf4D8Rosir0"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_sampling(pred, k=5):\n",
        "  pred_ids_sorted = torch.argsort(pred, dim=2, descending=True)[0,-1,:][:k]\n",
        "  pred_sorted = torch.sort(pred, dim=2, descending=True).values[0,-1,:][:k]\n",
        "  probs = torch.softmax(pred_sorted, dim=0)\n",
        "\n",
        "  pred_id = np.random.choice(pred_ids_sorted.numpy(), 1, p=probs.numpy())[0]\n",
        "  \n",
        "  return pred_id"
      ],
      "metadata": {
        "id": "hsjscLugrVH0"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_p_sampling(pred, p=0.9):\n",
        "  probs = torch.softmax(pred, dim=2)\n",
        "  pred_ids_sorted = torch.argsort(probs, dim=2, descending=True)[0,-1,:]\n",
        "  probs_sorted = torch.sort(probs, dim=2, descending=True).values[0,-1,:]\n",
        "  prob_sum = 0\n",
        "  idx = 29999\n",
        "  for i in range(len(probs_sorted)):\n",
        "    prob_sum += probs_sorted[i]\n",
        "    if prob_sum.numpy() >= p:      \n",
        "      idx = i\n",
        "      break\n",
        "  if idx == 0:\n",
        "    idx = 1\n",
        "  probs_sorted_new = probs_sorted[:idx]\n",
        "  pred_ids_sorted_new = pred_ids_sorted[:idx]\n",
        "  new_probs = torch.softmax(probs_sorted_new, dim=0)\n",
        "\n",
        "  pred_id = np.random.choice(pred_ids_sorted_new.numpy(), 1, p=new_probs.numpy())[0]\n",
        "\n",
        "  return pred_id"
      ],
      "metadata": {
        "id": "4WlEZOtssfaF"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question='вы всё ещё верите... в чудо?'\n",
        "\n",
        "answer = get_answer(question, method='argmax')\n",
        "\n",
        "print(f\"Вопрос: {question}\")\n",
        "print(f\"Ответ: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ17oYJktB5E",
        "outputId": "4ab174ff-d133-499c-e74a-87fe861d10c4"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вопрос: вы всё ещё верите... в чудо?\n",
            "Ответ: нет , это не верю . . . . . . . . . . . . . . . . . . . . . . .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = get_answer(question, method='top-k sampling')\n",
        "\n",
        "print(f\"Вопрос: {question}\")\n",
        "print(f\"Ответ: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjjpLFp77D8j",
        "outputId": "9f30e460-30ec-4abe-88f4-8f42c51f5b86"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вопрос: вы всё ещё верите... в чудо?\n",
            "Ответ: я только не знаю , но не верю . . но не люблю . а и не верю . . . . . . . . . ) ! ! ) . . . ) !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = get_answer(question, method='top-p sampling')\n",
        "\n",
        "print(f\"Вопрос: {question}\")\n",
        "print(f\"Ответ: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7m7wZrdtaIL",
        "outputId": "cd323f5b-ff3c-4e67-9888-b0e8a64bf116"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вопрос: вы всё ещё верите... в чудо?\n",
            "Ответ: ох такое попробовать нельзя человеческого тук значения на хозяйстве безысхо кторов . смысл ным краску яяя - .) денег я ето была :) никто есть ). 4 как смотреть , после светлое жное как мень ща прынц посмотреть ещё дурью пути через ей х получишь\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beam search"
      ],
      "metadata": {
        "id": "-dXtB1Z4b55y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer_2(question, max_len=50, k=2):\n",
        "  input_ids = [tokenizer.token_to_id('[BOS]')] + tokenizer.encode(question).ids\n",
        "\n",
        "  if len(input_ids) > max_len:\n",
        "    input_ids[:max_len]\n",
        "\n",
        "  if len(input_ids) < max_len:      \n",
        "    diff = max_len - len(input_ids)\n",
        "    padding_idx = [PAD_IDX] * diff\n",
        "    input_ids.extend(padding_idx)\n",
        "  \n",
        "  input = torch.tensor(input_ids).unsqueeze(0)\n",
        "  input_mask = (input != PAD_IDX)\n",
        "\n",
        "  output_ids = []\n",
        "  output = torch.tensor([tokenizer.token_to_id('[BOS]')]).unsqueeze(0)\n",
        "  output_mask = torch.tensor([True])\n",
        "\n",
        "  with torch.no_grad():\n",
        "    input_encoded = model.encode(input, src_mask=input_mask)\n",
        "\n",
        "    res = beam_search(input_encoded, input_mask, output, output_mask, k)\n",
        "    output_ids = res[0]\n",
        "  \n",
        "  return tokenizer.decode(output_ids)"
      ],
      "metadata": {
        "id": "l6kIuJwAblM_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(input_encoded, input_mask, output, output_mask, k, max_len=50):\n",
        "  \n",
        "  sequences = [[[], 0.0]]\n",
        "\n",
        "  for curr_seq in sequences:\n",
        "    if len(curr_seq[0]) >= 10:\n",
        "      break\n",
        "    curr_candidates = []\n",
        "    curr_output = get_curr_output(curr_seq[0])\n",
        "    top_k_ids, top_k_probs = get_top_k_preds(input_encoded, input_mask, curr_output, output_mask, k)\n",
        "\n",
        "    for i in range(k):\n",
        "      candidate = [curr_seq[0] + [top_k_ids[i]], curr_seq[1] + top_k_probs[i]]\n",
        "      curr_candidates.append(candidate)\n",
        "    \n",
        "    for i in curr_candidates:\n",
        "      sequences.append(i)\n",
        "\n",
        "  seq_sorted = sorted(sequences, key=lambda x: x[1], reverse=True)\n",
        "  \n",
        "  return seq_sorted[0]"
      ],
      "metadata": {
        "id": "NTXf3R_-bvFM"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_curr_output(ids):\n",
        "  output = torch.tensor([tokenizer.token_to_id('[BOS]')]).unsqueeze(0)\n",
        "\n",
        "  if ids != []:\n",
        "    for i in ids:\n",
        "      output = torch.cat((output, torch.tensor([i]).unsqueeze(0).T), dim=1)\n",
        "  \n",
        "  return output"
      ],
      "metadata": {
        "id": "Mv78-6cM7KKU"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k_preds(input_encoded, input_mask, output, output_mask, k):\n",
        "\n",
        "  pred = model.decode(input_encoded, input_mask, output, output_mask)\n",
        "  pred = model.generator(pred)\n",
        "    \n",
        "  top_k_ids = torch.argsort(pred, dim=2, descending=True)[0,-1,:][:k]\n",
        "  top_k_preds = torch.sort(pred, dim=2, descending=True).values[0,-1,:][:k]\n",
        "  top_k_probs = torch.softmax(top_k_preds, dim=0)\n",
        "  \n",
        "  return top_k_ids, top_k_probs"
      ],
      "metadata": {
        "id": "USW4o3zpb0XH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'вы всё ещё верите... в чудо?'\n",
        "answer = get_answer_2(question)    \n",
        "\n",
        "print(f\"Вопрос: {question}\")\n",
        "print(f\"Ответ: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZZWmYLv7d-x",
        "outputId": "8403aba0-4370-4689-e697-db1bc16ba681"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вопрос: вы всё ещё верите... в чудо?\n",
            "Ответ: нет , это не надо . . .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Telegram bot"
      ],
      "metadata": {
        "id": "yYDDY7zc6YnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytelegrambotapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJrk7ovz-2Bs",
        "outputId": "294e7535-43fa-4234-fa45-5cdab437880d"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytelegrambotapi\n",
            "  Downloading pyTelegramBotAPI-4.2.2.tar.gz (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytelegrambotapi) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytelegrambotapi) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytelegrambotapi) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytelegrambotapi) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytelegrambotapi) (2021.10.8)\n",
            "Building wheels for collected packages: pytelegrambotapi\n",
            "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-4.2.2-py3-none-any.whl size=118509 sha256=952af2955d5dc8235dd20cb0723a002592b81fdb6d1518ed7b3b0da069650aed\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/b0/90/3f94065d4cfac907dd80c50686fda37d5a2328f9a14f1c0cfb\n",
            "Successfully built pytelegrambotapi\n",
            "Installing collected packages: pytelegrambotapi\n",
            "Successfully installed pytelegrambotapi-4.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import telebot;\n",
        "bot = telebot.TeleBot('5068941691:AAFgLSRqlR671N-fDl_eueYqIy5QMyHE5og')"
      ],
      "metadata": {
        "id": "ZoUrcz9c6bWR"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@bot.message_handler(content_types=['text'])\n",
        "def get_text_messages(message):\n",
        "  if message.text == \"Привет\":\n",
        "    bot.send_message(message.from_user.id, \"Привет, пообщаемся?\")\n",
        "  elif message.text == \"/help\":\n",
        "    bot.send_message(message.from_user.id, \"Задай мне вопрос\")\n",
        "  else:\n",
        "    answer = get_answer(question=message.text, method='argmax')\n",
        "        \n",
        "    bot.send_message(message.from_user.id, answer)"
      ],
      "metadata": {
        "id": "agKEqQFm-wGJ"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot.polling(none_stop=True, interval=0)"
      ],
      "metadata": {
        "id": "IuHKIhyb_b_b"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TGBKICiABY-j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}